<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="SVD（Singular Value Decomposition）的直观理解一、SVD是什么（一）、SVD的基本概念在机器学习中，通常会进行数据压缩、噪声去除等。而奇异值分解（SVD）在这里主要用来进行降维。当矩阵非常大，数据非常多，计算会变得困难，在这里我们可以使用SVD。SVD保留了矩阵原有的结构特征。 奇异值分解可以写为$$M&#x3D;U\Sigma V^T$$任意矩阵$M$（非零的$m">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2023/08/27/SVD%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="SVD（Singular Value Decomposition）的直观理解一、SVD是什么（一）、SVD的基本概念在机器学习中，通常会进行数据压缩、噪声去除等。而奇异值分解（SVD）在这里主要用来进行降维。当矩阵非常大，数据非常多，计算会变得困难，在这里我们可以使用SVD。SVD保留了矩阵原有的结构特征。 奇异值分解可以写为$$M&#x3D;U\Sigma V^T$$任意矩阵$M$（非零的$m">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/.%5Ctemp%5C1653822776843(1).png">
<meta property="og:image" content="http://example.com/.%5Ctemp%5C1653822885744(1).png">
<meta property="og:image" content="http://example.com/.%5Ctemp%5C1653820662599(1).png">
<meta property="og:image" content="http://example.com/.%5Ctemp%5C1653816064588.png">
<meta property="article:published_time" content="2023-08-27T12:31:04.054Z">
<meta property="article:modified_time" content="2022-05-29T15:20:40.278Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/.%5Ctemp%5C1653822776843(1).png">

<link rel="canonical" href="http://example.com/2023/08/27/SVD%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title> | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/27/SVD%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-08-27 20:31:04" itemprop="dateCreated datePublished" datetime="2023-08-27T20:31:04+08:00">2023-08-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-29 23:20:40" itemprop="dateModified" datetime="2022-05-29T23:20:40+08:00">2022-05-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="SVD（Singular-Value-Decomposition）的直观理解"><a href="#SVD（Singular-Value-Decomposition）的直观理解" class="headerlink" title="SVD（Singular Value Decomposition）的直观理解"></a>SVD（Singular Value Decomposition）的直观理解</h1><h2 id="一、SVD是什么"><a href="#一、SVD是什么" class="headerlink" title="一、SVD是什么"></a>一、SVD是什么</h2><h3 id="（一）、SVD的基本概念"><a href="#（一）、SVD的基本概念" class="headerlink" title="（一）、SVD的基本概念"></a>（一）、SVD的基本概念</h3><p>在机器学习中，通常会进行数据压缩、噪声去除等。而奇异值分解（SVD）在这里主要用来进行降维。当矩阵非常大，数据非常多，计算会变得困难，在这里我们可以使用SVD。SVD保留了矩阵原有的结构特征。</p>
<p>奇异值分解可以写为<br>$$<br>M&#x3D;U\Sigma V^T<br>$$<br>任意矩阵$M$（非零的$m \times n$矩阵）可以分解为3个实矩阵的相乘。</p>
<p>其中$U$是$m$阶正交矩阵，<br>$$<br>U&#x3D; \left[<br>\matrix{<br>  u_1 &amp; u_2 &amp; … &amp; u_m<br>}<br>\right]<br>$$<br>$V$是$n$阶正交矩阵，<br>$$<br>V&#x3D; \left[<br>\matrix{<br>  v_1 &amp; v_2 &amp; … &amp; v_n<br>}<br>\right]<br>$$<br>$\Sigma$是由降序排列的非负的对角线元素组成的$m \times n$矩形对角矩阵（$\Sigma&#x3D;diag(\sigma_1,\sigma_2,\cdot\cdot\cdot,\sigma_p)，p&#x3D;min(m,n)$）。</p>
<p>$U\Sigma V^T$称为矩阵M的奇异值分解（SVD）,$\sigma_i$称为矩阵M的奇异值，$U$的列向量称为左奇异向量，$V$的列向量称为右奇异向量。</p>
<h1 id="例如：-left-matrix-x-1-y-1-x-2-y-2-x-3-y-3-x-4-y-4-right"><a href="#例如：-left-matrix-x-1-y-1-x-2-y-2-x-3-y-3-x-4-y-4-right" class="headerlink" title="例如：$$\left[\matrix{  x_1 &amp; y_1\  x_2 &amp; y_2\  x_3 &amp; y_3\  x_4 &amp; y_4}\right]"></a>例如：<br>$$<br>\left[<br>\matrix{<br>  x_1 &amp; y_1\<br>  x_2 &amp; y_2\<br>  x_3 &amp; y_3\<br>  x_4 &amp; y_4<br>}<br>\right]</h1><p> \left[<br>\matrix{</p>
<ul>
<li>&amp; - &amp; - &amp; -\</li>
<li>&amp; - &amp; - &amp; -\</li>
<li>&amp; - &amp; - &amp; -\</li>
<li>&amp; - &amp; - &amp; -<br>}<br>\right]<br> \left[<br>\matrix{<br>  \sigma_1 &amp; 0\<br>  0 &amp; \sigma_2 \<br>  0 &amp; 0\<br>  0 &amp; 0<br>}<br>\right]<br> \left[<br>\matrix{</li>
<li>&amp; -  \</li>
<li>&amp; - \<br>}<br>\right]<br>$$<br><em>注：我们知道一组正交和归一化的向量称为正交集。$V$的列向量集合就是一个正交集（$U$的列向量也是同理）。</em></li>
</ul>
<h3 id="（二）、求SVD分解"><a href="#（二）、求SVD分解" class="headerlink" title="（二）、求SVD分解"></a>（二）、求SVD分解</h3><p>$$<br>M&#x3D;U\Sigma V^T\<br>M^T M&#x3D;(U\Sigma V^T)^TU\Sigma V^T \<br>M^T M&#x3D;V \Sigma U^T   U \Sigma V^T \<br>M^T M &#x3D; V \Sigma  \Sigma V^T \<br>L &#x3D;  \Sigma  \Sigma \quad  \Sigma &#x3D;\left[<br>\matrix{<br>  \sigma_1 &amp; 0  \<br>  0 &amp; \sigma_2 \<br>}<br>\right] \<br>M^T M &#x3D; V L V^T \<br>M^T M V &#x3D; V L \quad M M^TU &#x3D; UL \<br>L &#x3D;\left[<br>\matrix{<br>  \sigma_1^2 &amp; 0  \<br>  0 &amp; \sigma_2^2 \<br>}<br>\right] &#x3D;\left[<br>\matrix{<br>  \lambda_1 &amp; 0  \<br>  0 &amp; \lambda_2 \<br>}<br>\right]<br>$$</p>
<p>$V$是$M^T M$的特征向量，$U$是$M M^T$的特征向量，$\Sigma$是$M^T M$和$M M^T$的特征值的开方构成的对角阵。这就是SVD的分解过程。</p>
<p><em>那么奇异值和特征值是如何对应的呢？</em></p>
<p>根据上面的推导过程可知，我们将矩阵$M$的转置*$M$，会得到一个方阵，再对方阵求特征值可以的到：<br>$$<br>(M^TM)v_i&#x3D;\lambda_iv_i<br>$$<br>上式的$v$就是右奇异向量。此外，<br>$$<br>\sigma_i&#x3D;\sqrt{\lambda_i}<br>$$</p>
<p>$$<br>u_i&#x3D;\frac{1}{\sigma_i}Av_i<br>$$</p>
<p>式中的$\sigma$就是奇异值，$u$就是左奇异向量。</p>
<p>奇异值$\sigma$和特征值类似，在矩阵$\Sigma$中也是从大到小排列，且σ从大到小减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占到了所有奇异值之和的99%以上了，其他的可以看作是0，这时我们将这些为0的奇异值所在的行全部删除（维度发生退化）。也就是说，我们也可以用前r大的奇异值来近似描述矩阵。（可以将奇异值简单理解为主特征值）</p>
<p>这时：<br>$$<br>M_{m \times n} \approx U_{m \times r} \Sigma_{r \times r} V^T_{r \times n}<br>$$<br>$r$是一个远小于m、n的数，当r越接近于n，右边三个矩阵相乘的结果越接近矩阵$M$。这样我们可以用三个小的矩阵来代替原来的大矩阵，减少存储空间，实现数据压缩。 因此r的选取十分重要。</p>
<h2 id="二、直观理解SVD"><a href="#二、直观理解SVD" class="headerlink" title="二、直观理解SVD"></a>二、直观理解SVD</h2><p>在我们了解了SVD的基本概念后，可能还是不懂到底是怎么一回事。为什么可以这么分解？不要着急，我们先回顾一下基本的线性代数，接下来我们将会用到。</p>
<h3 id="（一）、基础知识"><a href="#（一）、基础知识" class="headerlink" title="（一）、基础知识"></a>（一）、基础知识</h3><h4 id="1-旋转"><a href="#1-旋转" class="headerlink" title="1.旋转"></a>1.旋转</h4><p>在二维空间中，旋转矩阵可以定义为：<br>$$<br>A&#x3D;\left[<br>\matrix{<br>  cos(\theta) &amp; -sin(\theta)  \<br>  sin(\theta) &amp;  cos(\theta) \<br>}<br>\right]<br>$$<br>该矩阵可以将向量围绕原点旋转$\theta$度。</p>
<p><img src="/.%5Ctemp%5C1653822776843(1).png" alt="1653822776843(1)"></p>
<h4 id="2-拉伸"><a href="#2-拉伸" class="headerlink" title="2.拉伸"></a>2.拉伸</h4><p>在二维空间中，拉伸矩阵可以定义为：<br>$$<br>B&#x3D;\left[<br>\matrix{<br>  k &amp; 0  \<br>  0 &amp; 1 \<br>}<br>\right]<br>$$<br>该矩阵可以将向量沿着$x$轴方向拉伸$k$倍，但不影响$y$方向。</p>
<p><img src="/.%5Ctemp%5C1653822885744(1).png" alt="1653822885744(1)"></p>
<p><em><strong>有没有发现旋转矩阵矩阵和拉伸矩阵的形式与SVD的矩阵有些相像？</strong></em></p>
<h3 id="（二）、从矩阵分解的角度来理解SVD"><a href="#（二）、从矩阵分解的角度来理解SVD" class="headerlink" title="（二）、从矩阵分解的角度来理解SVD"></a>（二）、从矩阵分解的角度来理解SVD</h3><p>在我们回顾了旋转和拉伸后，我们再来重新回顾SVD的概念。<br>$$<br>M&#x3D;U\Sigma V^T<br>$$<br>这时我们可以把$U$看作是旋转矩阵（$U$和$A$一样都是正交矩阵），$\Sigma$看作是拉伸矩阵($\Sigma$只有对角线上有数字)，$V^T$也看作是旋转矩阵（$V^T$和A一样是正交矩阵）。</p>
<p>这样我们就<strong>把矩阵$M$代表的复杂线性变换（既不是旋转也不是拉伸）分解为旋转加拉伸（在维度变化的情况下还有投影）</strong>！</p>
<p><img src="/.%5Ctemp%5C1653820662599(1).png" alt="1653820662599(1)"></p>
<p>首先$V^T$将单位正交向量旋转，然后$\Sigma$对这对正交向量进行放缩，最后$U$再次将放缩后的向量进行旋转，其结果与直接进行M变换相同。</p>
<h3 id="（三）、从基变换的角度来理解SVD"><a href="#（三）、从基变换的角度来理解SVD" class="headerlink" title="（三）、从基变换的角度来理解SVD"></a>（三）、从基变换的角度来理解SVD</h3><p>在2x2矩阵的情况下：</p>
<p><img src="/.%5Ctemp%5C1653816064588.png" alt="1653816064588"></p>
<p>​                         $V&#x3D;[\vec{v_1} \quad \vec{v_2}]$                                                                                  $U&#x3D;[\vec{u_1} \quad \vec{u_2}]$</p>
<p>如果我们能找到一组正交基，经过M线性变换，正交基从左边变为右边，它的长度和方向都被改变，但仍然保持垂直，即$MV&#x3D;U\Sigma&#x3D;[\sigma_1 \vec{u_1} \quad \sigma_2 \vec{u_2}]$，其中<br>$$<br>\Sigma&#x3D;\left[<br>\matrix{<br>  \sigma_1 &amp; 0  \<br>  0 &amp; \sigma_2 \<br>}<br>\right]<br>$$<br>可知，<br>$$<br>M&#x3D;U\Sigma V^T<br>$$<br>在这里$V$就是原始域的标准正交基，$U$是经过$M$变换后的标准正交基。</p>
<h3 id="（四）、从协方差角度理解SVD的数据压缩"><a href="#（四）、从协方差角度理解SVD的数据压缩" class="headerlink" title="（四）、从协方差角度理解SVD的数据压缩"></a>（四）、从协方差角度理解SVD的数据压缩</h3><p>两个变量的关系可以用协方差描述：<br>$$<br>cov(x,y)&#x3D;\frac{1}{n-1} \sum^{n}_{i&#x3D;1}(x_i- \bar{x})(y_i- \bar{y})<br>$$<br>cov(x,y)&gt;0时，两个变量呈正相关，cov(x,y)&lt;0时，两个变量呈负相关，cov(x,y)&#x3D;0时，两个变量呈不相关。</p>
<p>多个变量间的关系可以通过协方差矩阵描述：<br>$$<br>cov(x,y,z)&#x3D;\left[<br>\matrix{<br> cov(x,x)  &amp; cov(x,y) &amp; cov(x,z)  \<br>  cov(y,x)  &amp; cov(y,y) &amp; cov(y,z) \<br>  cov(z,x)  &amp; cov(z,y) &amp; cov(z,z)<br>}<br>\right]<br>$$<br>相关系数公式：<br>$$<br>\rho&#x3D;\frac{cov(x,y)}{\sqrt{D(x)}\sqrt{D(y)}}<br>$$<br>因此，从物理意义上讲，协方差描述变量之间的相互关系程度，协方差矩阵计算各维度之间的相关性。</p>
<h1 id="假设数据集为n维，共m个数据，每一行就代表一个数据，有：-M-left-matrix-（x-1-T-（x-2-T-（x-m-T-right-（x-i-表示第i个样本，-X-j-表示第j维特征，-x-i-j-表示第i个样本的第j维特征。-M-T-M-left-x-1-x-2-…-x-m-right-left-matrix-（x-1-T-（x-2-T-（x-m-T-right"><a href="#假设数据集为n维，共m个数据，每一行就代表一个数据，有：-M-left-matrix-（x-1-T-（x-2-T-（x-m-T-right-（x-i-表示第i个样本，-X-j-表示第j维特征，-x-i-j-表示第i个样本的第j维特征。-M-T-M-left-x-1-x-2-…-x-m-right-left-matrix-（x-1-T-（x-2-T-（x-m-T-right" class="headerlink" title="假设数据集为n维，共m个数据，每一行就代表一个数据，有：$$M&#x3D;\left[\matrix{  （x^{(1)})^T   \  （x^{(2)})^T   \  .  \  .\  .\  （x^{(m)})^T}\right]$$$（x^{(i)})$表示第i个样本，$X_j$表示第j维特征，$x^{(i)}_j$表示第i个样本的第j维特征。$$M^T M&#x3D;\left[x^{(1)},x^{(2)},…,x^{(m)}\right]\left[\matrix{  （x^{(1)})^T   \  （x^{(2)})^T   \  .  \  .\  .\  （x^{(m)})^T}\right]"></a>假设数据集为n维，共m个数据，每一行就代表一个数据，有：<br>$$<br>M&#x3D;\left[<br>\matrix{<br>  （x^{(1)})^T   \<br>  （x^{(2)})^T   \<br>  .  \<br>  .\<br>  .\<br>  （x^{(m)})^T<br>}<br>\right]<br>$$<br>$（x^{(i)})$表示第i个样本，$X_j$表示第j维特征，$x^{(i)}_j$表示第i个样本的第j维特征。<br>$$<br>M^T M&#x3D;\left[<br>x^{(1)},x^{(2)},…,x^{(m)}<br>\right]<br>\left[<br>\matrix{<br>  （x^{(1)})^T   \<br>  （x^{(2)})^T   \<br>  .  \<br>  .\<br>  .\<br>  （x^{(m)})^T<br>}<br>\right]</h1><p>x^{(1)}（x^{(1)})^T + x^{(2)}（x^{(2)})^T+…+x^{(m)}（x^{(m)})^T\<br>\Rightarrow M^T M &#x3D;\left[<br>\matrix{<br> cov(x_1,x_1)  &amp; cov(x_1,x_2) &amp; … &amp; cov(x_1,x_n)  \<br>  cov(x_2,x_1)  &amp; cov(x_2,x_2) &amp; … &amp; cov(x_2,x_n)  \<br>  .&amp;.&amp;&amp;.\<br>   .&amp;.&amp;&amp;.\<br>    .&amp;.&amp;&amp;.\<br>cov(x_n,x_1)  &amp; cov(x_n,x_2) &amp; … &amp; cov(x_n,x_n)<br>}<br>\right]<br>$$<br>因此，我们知道$M^T M$描述各特征之间的相关关系，所以$M^T M$的正交基$V$是以数据集的特征空间进行展开的。</p>
<p>数据集M在特征空间展开为：<br>$$<br>X_{M<em>N} &#x3D; M_{M</em>N}V_{N<em>N}<br>$$<br>当我们选择前r个特征值来表示原始数据集，那么数据集M在特征空间展开为：<br>$$<br>X_{M</em>r} &#x3D; M_{M<em>N}V_{N</em>r}<br>$$<br>上式对列进行了降维，即右奇异矩阵可以用于对列数的压缩。</p>
<p>同理，$M M^T$描述样本数据间的相关关系，左奇异矩阵U是一样本空间进行展开。当我们选择前r个特征值来表示原始数据集，那么数据集M在样本空间展开为：<br>$$<br>Y_{r<em>N} &#x3D; U^T_{r</em>M}M_{M*N}<br>$$<br>上式对行进行了降维，即左奇异矩阵可以用于对行数的压缩。</p>
<h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>现在我们理解了奇异值分解其实就是一种矩阵近似的方法，而这种近似是在佛罗贝尔乌斯范数（平方损失）意义下的最优近似，即数据压缩。它保留了数据的结构特征，但大大压缩了数据量。</p>
<p>实际上，可以把奇异向量看作是对M的信息的编码，把奇异值看作是表达这些信息的重要性。$\Sigma$可以看作是$V$和$U$之间的桥梁，选取的奇异值越多，那么桥越宽，$V$和$U$能够交流的信息就越多，最后得到的矩阵就越接近M。</p>
<p>参考文献：</p>
<p>[1] 李航.统计学习方法[M]. 北京：清华大学出版社，2012</p>
<p>[2] David Austin.We Recommend a Singular Value Decomposition.2009.</p>
<p><a target="_blank" rel="noopener" href="http://www.ams.org/publicoutreach/feature-column/fcarc-svd">http://www.ams.org/publicoutreach/feature-column/fcarc-svd</a></p>
<p>[3] 奇异值的物理意义是什么？ - 知乎用户的回答 - 知乎.2020<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/22237507/answer/801023732">https://www.zhihu.com/question/22237507/answer/801023732</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/08/27/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#SVD%EF%BC%88Singular-Value-Decomposition%EF%BC%89%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="nav-number">1.</span> <span class="nav-text">SVD（Singular Value Decomposition）的直观理解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81SVD%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">1.1.</span> <span class="nav-text">一、SVD是什么</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E3%80%81SVD%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">1.1.1.</span> <span class="nav-text">（一）、SVD的基本概念</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BE%8B%E5%A6%82%EF%BC%9A-left-matrix-x-1-y-1-x-2-y-2-x-3-y-3-x-4-y-4-right"><span class="nav-number">2.</span> <span class="nav-text">例如：$$\left[\matrix{  x_1 &amp; y_1\  x_2 &amp; y_2\  x_3 &amp; y_3\  x_4 &amp; y_4}\right]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E3%80%81%E6%B1%82SVD%E5%88%86%E8%A7%A3"><span class="nav-number">2.0.1.</span> <span class="nav-text">（二）、求SVD分解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3SVD"><span class="nav-number">2.1.</span> <span class="nav-text">二、直观理解SVD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E3%80%81%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="nav-number">2.1.1.</span> <span class="nav-text">（一）、基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E6%97%8B%E8%BD%AC"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">1.旋转</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%8B%89%E4%BC%B8"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">2.拉伸</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E3%80%81%E4%BB%8E%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E7%9A%84%E8%A7%92%E5%BA%A6%E6%9D%A5%E7%90%86%E8%A7%A3SVD"><span class="nav-number">2.1.2.</span> <span class="nav-text">（二）、从矩阵分解的角度来理解SVD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%88%E4%B8%89%EF%BC%89%E3%80%81%E4%BB%8E%E5%9F%BA%E5%8F%98%E6%8D%A2%E7%9A%84%E8%A7%92%E5%BA%A6%E6%9D%A5%E7%90%86%E8%A7%A3SVD"><span class="nav-number">2.1.3.</span> <span class="nav-text">（三）、从基变换的角度来理解SVD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%88%E5%9B%9B%EF%BC%89%E3%80%81%E4%BB%8E%E5%8D%8F%E6%96%B9%E5%B7%AE%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3SVD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9"><span class="nav-number">2.1.4.</span> <span class="nav-text">（四）、从协方差角度理解SVD的数据压缩</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BAn%E7%BB%B4%EF%BC%8C%E5%85%B1m%E4%B8%AA%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%AF%8F%E4%B8%80%E8%A1%8C%E5%B0%B1%E4%BB%A3%E8%A1%A8%E4%B8%80%E4%B8%AA%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%9C%89%EF%BC%9A-M-left-matrix-%EF%BC%88x-1-T-%EF%BC%88x-2-T-%EF%BC%88x-m-T-right-%EF%BC%88x-i-%E8%A1%A8%E7%A4%BA%E7%AC%ACi%E4%B8%AA%E6%A0%B7%E6%9C%AC%EF%BC%8C-X-j-%E8%A1%A8%E7%A4%BA%E7%AC%ACj%E7%BB%B4%E7%89%B9%E5%BE%81%EF%BC%8C-x-i-j-%E8%A1%A8%E7%A4%BA%E7%AC%ACi%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%9A%84%E7%AC%ACj%E7%BB%B4%E7%89%B9%E5%BE%81%E3%80%82-M-T-M-left-x-1-x-2-%E2%80%A6-x-m-right-left-matrix-%EF%BC%88x-1-T-%EF%BC%88x-2-T-%EF%BC%88x-m-T-right"><span class="nav-number">3.</span> <span class="nav-text">假设数据集为n维，共m个数据，每一行就代表一个数据，有：$$M&#x3D;\left[\matrix{  （x^{(1)})^T   \  （x^{(2)})^T   \  .  \  .\  .\  （x^{(m)})^T}\right]$$$（x^{(i)})$表示第i个样本，$X_j$表示第j维特征，$x^{(i)}_j$表示第i个样本的第j维特征。$$M^T M&#x3D;\left[x^{(1)},x^{(2)},…,x^{(m)}\right]\left[\matrix{  （x^{(1)})^T   \  （x^{(2)})^T   \  .  \  .\  .\  （x^{(m)})^T}\right]</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E6%80%BB%E7%BB%93"><span class="nav-number">3.1.</span> <span class="nav-text">三、总结</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
